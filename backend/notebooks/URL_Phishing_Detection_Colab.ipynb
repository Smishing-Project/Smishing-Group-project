{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# URL í”¼ì‹± íƒì§€ ëª¨ë¸ í•™ìŠµ\n",
    "## Google Colabìš© ë…¸íŠ¸ë¶\n",
    "\n",
    "**ëª©í‘œ:** Random Forest ëª¨ë¸ë¡œ ì•…ì„± URL íƒì§€ (Accuracy 85%+, Recall 90%+)\n",
    "\n",
    "**ì‘ì„±ì:** ì´ê²½ì¤€  \n",
    "**í”„ë¡œì íŠ¸:** ìŠ¤ë¯¸ì‹± íƒì§€ ì‹œìŠ¤í…œ (OCR/QR + URL ìœ„í—˜ íŒë‹¨)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
    "!pip install tldextract python-whois scikit-learn pandas numpy joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "import tldextract\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score\n",
    ")\n",
    "import joblib\n",
    "\n",
    "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ìƒ˜í”Œ ë°ì´í„°ì…‹ ìƒì„±\n",
    "\n",
    "ì‹¤ì œ í”„ë¡œì íŠ¸ì—ì„œëŠ” PhishTank, URLhaus ë“±ì—ì„œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì•…ì„± URL ìƒ˜í”Œ\n",
    "malicious_urls = [\n",
    "    \"http://paypal-secure-login.com/verify\",\n",
    "    \"http://amazon-account-verify.com/update\",\n",
    "    \"http://apple-id-locked.com/unlock\",\n",
    "    \"http://netflix-payment-failed.com/update\",\n",
    "    \"http://google-security-alert.com/verify\",\n",
    "    \"http://microsoft-account-suspended.com/restore\",\n",
    "    \"http://facebook-unusual-activity.com/verify\",\n",
    "    \"http://instagram-verify-account.com/confirm\",\n",
    "    \"http://cj-delivery-notice.com/parcel\",\n",
    "    \"http://hanjin-package-arrived.com/check\",\n",
    "    \"http://post-office-notice.com/delivery\",\n",
    "    \"http://fedex-customs-clearance.com/pay\",\n",
    "    \"http://192.168.1.100/admin/login\",\n",
    "    \"http://203.45.67.89/secure/verify\",\n",
    "    \"http://10.0.0.1/banking/update\",\n",
    "    \"http://bit.ly/free-iphone-giveaway\",\n",
    "    \"http://t.co/win-prize-now\",\n",
    "    \"http://secure-banking-update-urgent.com\",\n",
    "    \"http://verify-your-account-now-2024.com\",\n",
    "    \"http://claim-your-reward-immediately.com\",\n",
    "    \"http://urgent-action-required-today.com\",\n",
    "    \"http://limited-time-offer-act-now.com\",\n",
    "    \"http://congratulations-winner-click-here.com\",\n",
    "    \"http://account-locked-verify-identity.com\",\n",
    "    \"http://suspicious-activity-confirm-now.com\",\n",
    "    \"http://pay-pal-secure-login-verify.com\",\n",
    "    \"http://face-book-account-recovery.com\",\n",
    "    \"http://goo-gle-security-check.com\",\n",
    "    \"http://paypa1-secure123.com\",\n",
    "    \"http://amaz0n-verify456.com\",\n",
    "]\n",
    "\n",
    "# ì •ìƒ URL ìƒ˜í”Œ\n",
    "benign_urls = [\n",
    "    \"https://www.google.com\",\n",
    "    \"https://www.youtube.com\",\n",
    "    \"https://www.facebook.com\",\n",
    "    \"https://www.amazon.com\",\n",
    "    \"https://www.wikipedia.org\",\n",
    "    \"https://www.twitter.com\",\n",
    "    \"https://www.instagram.com\",\n",
    "    \"https://www.linkedin.com\",\n",
    "    \"https://www.reddit.com\",\n",
    "    \"https://www.netflix.com\",\n",
    "    \"https://www.naver.com\",\n",
    "    \"https://www.daum.net\",\n",
    "    \"https://www.kakao.com\",\n",
    "    \"https://www.coupang.com\",\n",
    "    \"https://www.11st.co.kr\",\n",
    "    \"https://www.gmarket.co.kr\",\n",
    "    \"https://www.github.com\",\n",
    "    \"https://www.stackoverflow.com\",\n",
    "    \"https://www.medium.com\",\n",
    "    \"https://www.bbc.com\",\n",
    "    \"https://www.cnn.com\",\n",
    "    \"https://www.nytimes.com\",\n",
    "    \"https://www.ebay.com\",\n",
    "    \"https://www.walmart.com\",\n",
    "]\n",
    "\n",
    "# ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
    "df = pd.DataFrame({\n",
    "    'url': malicious_urls + benign_urls,\n",
    "    'label': [1] * len(malicious_urls) + [0] * len(benign_urls)\n",
    "})\n",
    "\n",
    "# ì…”í”Œ\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"âœ… ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ\")\n",
    "print(f\"   - ì „ì²´: {len(df)}ê°œ\")\n",
    "print(f\"   - ì•…ì„±: {(df['label']==1).sum()}ê°œ\")\n",
    "print(f\"   - ì •ìƒ: {(df['label']==0).sum()}ê°œ\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. íŠ¹ì§• ì¶”ì¶œ í•¨ìˆ˜ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(url):\n",
    "    \"\"\"URLì—ì„œ 30ê°œ íŠ¹ì§• ì¶”ì¶œ\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    try:\n",
    "        parsed = urlparse(url)\n",
    "        extracted = tldextract.extract(url)\n",
    "        domain = parsed.netloc\n",
    "        \n",
    "        # 1-10: URL êµ¬ì¡°\n",
    "        features['url_length'] = len(url)\n",
    "        features['domain_length'] = len(domain)\n",
    "        features['path_length'] = len(parsed.path)\n",
    "        features['hyphen_count'] = url.count('-')\n",
    "        features['underscore_count'] = url.count('_')\n",
    "        features['slash_count'] = url.count('/')\n",
    "        features['dot_count'] = url.count('.')\n",
    "        features['has_at_symbol'] = 1 if '@' in url else 0\n",
    "        features['digit_count'] = sum(c.isdigit() for c in url)\n",
    "        features['special_char_count'] = len(re.findall(r'[!#$%&*+=?^`{|}~]', url))\n",
    "        \n",
    "        # 11-20: ë„ë©”ì¸\n",
    "        features['is_ip_address'] = 1 if re.match(r'^(\\d{1,3}\\.){3}\\d{1,3}$', domain) else 0\n",
    "        subdomain = extracted.subdomain\n",
    "        features['subdomain_count'] = len(subdomain.split('.')) if subdomain else 0\n",
    "        features['domain_has_digits'] = 1 if any(c.isdigit() for c in domain) else 0\n",
    "        tld = extracted.suffix\n",
    "        trusted_tlds = ['com', 'org', 'net', 'edu', 'gov']\n",
    "        features['is_trusted_tld'] = 1 if tld in trusted_tlds else 0\n",
    "        features['has_www'] = 1 if domain.startswith('www.') else 0\n",
    "        features['domain_hyphen_count'] = domain.count('-')\n",
    "        \n",
    "        # ì—”íŠ¸ë¡œí”¼\n",
    "        if domain:\n",
    "            char_freq = Counter(domain)\n",
    "            entropy = 0.0\n",
    "            for count in char_freq.values():\n",
    "                prob = count / len(domain)\n",
    "                entropy -= prob * math.log2(prob)\n",
    "            features['domain_entropy'] = round(entropy, 4)\n",
    "        else:\n",
    "            features['domain_entropy'] = 0.0\n",
    "        \n",
    "        features['starts_with_digit'] = 1 if domain and domain[0].isdigit() else 0\n",
    "        features['domain_dot_count'] = domain.count('.')\n",
    "        features['is_long_domain'] = 1 if len(domain) > 15 else 0\n",
    "        \n",
    "        # 21-30: ì½˜í…ì¸ \n",
    "        features['is_https'] = 1 if parsed.scheme == 'https' else 0\n",
    "        features['has_port'] = 1 if ':' in domain and not domain.startswith('[') else 0\n",
    "        \n",
    "        suspicious_keywords = [\n",
    "            'login', 'signin', 'account', 'update', 'secure', 'banking',\n",
    "            'verify', 'confirm', 'password', 'urgent', 'suspended', 'locked'\n",
    "        ]\n",
    "        url_lower = url.lower()\n",
    "        features['suspicious_keyword_count'] = sum(1 for kw in suspicious_keywords if kw in url_lower)\n",
    "        \n",
    "        query_params = parsed.query.split('&') if parsed.query else []\n",
    "        features['query_param_count'] = len(query_params)\n",
    "        features['has_fragment'] = 1 if parsed.fragment else 0\n",
    "        \n",
    "        path_depth = len([p for p in parsed.path.split('/') if p])\n",
    "        features['path_depth'] = path_depth\n",
    "        \n",
    "        path = parsed.path\n",
    "        features['has_file_extension'] = 1 if '.' in path.split('/')[-1] else 0\n",
    "        features['has_repetitive_chars'] = 1 if re.search(r'(.)\\1{2,}', url) else 0\n",
    "        \n",
    "        # ë¸Œëœë“œ ë¶ˆì¼ì¹˜\n",
    "        brands = ['paypal', 'amazon', 'google', 'apple', 'microsoft', 'facebook']\n",
    "        brand_mismatch = 0\n",
    "        for brand in brands:\n",
    "            if brand in url_lower and f'{brand}.com' not in url_lower:\n",
    "                brand_mismatch = 1\n",
    "                break\n",
    "        features['brand_mismatch'] = brand_mismatch\n",
    "        \n",
    "        short_domains = ['bit.ly', 't.co', 'goo.gl', 'tinyurl.com']\n",
    "        features['is_shortened_url'] = 1 if any(d in url_lower for d in short_domains) else 0\n",
    "        \n",
    "    except Exception as e:\n",
    "        # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ê°’\n",
    "        for i in range(30):\n",
    "            if f'feature_{i}' not in features:\n",
    "                features[f'feature_{i}'] = 0\n",
    "    \n",
    "    return features\n",
    "\n",
    "print(\"âœ… íŠ¹ì§• ì¶”ì¶œ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ì „ì²´ ë°ì´í„°ì…‹ì— íŠ¹ì§• ì¶”ì¶œ ì ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ” íŠ¹ì§• ì¶”ì¶œ ì‹œì‘...\")\n",
    "\n",
    "features_list = []\n",
    "for idx, url in enumerate(df['url']):\n",
    "    features = extract_features(url)\n",
    "    features_list.append(features)\n",
    "    \n",
    "    if (idx + 1) % 10 == 0:\n",
    "        print(f\"   ì§„í–‰: {idx+1}/{len(df)}\")\n",
    "\n",
    "# DataFrameìœ¼ë¡œ ë³€í™˜\n",
    "X = pd.DataFrame(features_list)\n",
    "y = df['label']\n",
    "\n",
    "print(f\"\\nâœ… íŠ¹ì§• ì¶”ì¶œ ì™„ë£Œ\")\n",
    "print(f\"   - ìƒ˜í”Œ ìˆ˜: {len(X)}\")\n",
    "print(f\"   - íŠ¹ì§• ìˆ˜: {len(X.columns)}\")\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ë°ì´í„° ë¶„í• "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"âœ… ë°ì´í„° ë¶„í•  ì™„ë£Œ\")\n",
    "print(f\"   - Train: {len(X_train)}ê°œ\")\n",
    "print(f\"   - Test:  {len(X_test)}ê°œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ëª¨ë¸ í•™ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ¤– Random Forest ëª¨ë¸ í•™ìŠµ ì‹œì‘...\\n\")\n",
    "\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=20,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nâœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ëª¨ë¸ í‰ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì˜ˆì¸¡\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "# Train ì„±ëŠ¥\n",
    "print(\"[Train ì„±ëŠ¥]\")\n",
    "print(f\"Accuracy: {accuracy_score(y_train, y_pred_train):.4f}\")\n",
    "\n",
    "# Test ì„±ëŠ¥\n",
    "print(\"\\n[Test ì„±ëŠ¥]\")\n",
    "acc = accuracy_score(y_test, y_pred_test)\n",
    "prec = precision_score(y_test, y_pred_test)\n",
    "rec = recall_score(y_test, y_pred_test)\n",
    "f1 = f1_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Accuracy:  {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall:    {rec:.4f}\")\n",
    "print(f\"F1-Score:  {f1:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "print(\"\\n[Confusion Matrix]\")\n",
    "print(f\"TN: {cm[0][0]}  FP: {cm[0][1]}\")\n",
    "print(f\"FN: {cm[1][0]}  TP: {cm[1][1]}\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\n[Classification Report]\")\n",
    "print(classification_report(y_test, y_pred_test, target_names=['Benign', 'Malicious']))\n",
    "\n",
    "# ëª©í‘œ ë‹¬ì„±\n",
    "print(\"\\nğŸ¯ ëª©í‘œ ë‹¬ì„± ì—¬ë¶€:\")\n",
    "print(f\"   - Accuracy â‰¥ 85%: {'âœ…' if acc >= 0.85 else 'âŒ'} ({acc:.1%})\")\n",
    "print(f\"   - Recall â‰¥ 90%:   {'âœ…' if rec >= 0.90 else 'âŒ'} ({rec:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. íŠ¹ì§• ì¤‘ìš”ë„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"[ìƒìœ„ 10ê°œ ì¤‘ìš” íŠ¹ì§•]\")\n",
    "print(feature_importance.head(10))\n",
    "\n",
    "# ì‹œê°í™” (ì„ íƒ)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance.head(10)['feature'], feature_importance.head(10)['importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 10 Feature Importance')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ëª¨ë¸ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ì €ì¥\n",
    "joblib.dump(model, 'url_classifier.pkl')\n",
    "print(\"âœ… ëª¨ë¸ ì €ì¥: url_classifier.pkl\")\n",
    "\n",
    "# íŠ¹ì§• ì´ë¦„ ì €ì¥\n",
    "joblib.dump(list(X.columns), 'feature_names.pkl')\n",
    "print(\"âœ… íŠ¹ì§• ì´ë¦„ ì €ì¥: feature_names.pkl\")\n",
    "\n",
    "# ë©”íƒ€ë°ì´í„° ì €ì¥\n",
    "metadata = {\n",
    "    'model_type': 'RandomForest',\n",
    "    'n_features': len(X.columns),\n",
    "    'accuracy': acc,\n",
    "    'precision': prec,\n",
    "    'recall': rec,\n",
    "    'f1_score': f1\n",
    "}\n",
    "joblib.dump(metadata, 'metadata.pkl')\n",
    "print(\"âœ… ë©”íƒ€ë°ì´í„° ì €ì¥: metadata.pkl\")\n",
    "\n",
    "print(\"\\nğŸ“¥ ë‹¤ìš´ë¡œë“œí•  íŒŒì¼:\")\n",
    "print(\"   1. url_classifier.pkl\")\n",
    "print(\"   2. feature_names.pkl\")\n",
    "print(\"   3. metadata.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. í…ŒìŠ¤íŠ¸ (ìƒˆë¡œìš´ URL ì˜ˆì¸¡)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_url(url):\n",
    "    \"\"\"ìƒˆ URLì˜ ìœ„í—˜ë„ ì˜ˆì¸¡\"\"\"\n",
    "    features = extract_features(url)\n",
    "    features_df = pd.DataFrame([features])\n",
    "    \n",
    "    prediction = model.predict(features_df)[0]\n",
    "    proba = model.predict_proba(features_df)[0]\n",
    "    \n",
    "    result = \"ì•…ì„±\" if prediction == 1 else \"ì •ìƒ\"\n",
    "    confidence = proba[prediction] * 100\n",
    "    \n",
    "    print(f\"URL: {url}\")\n",
    "    print(f\"ì˜ˆì¸¡: {result} (ì‹ ë¢°ë„: {confidence:.1f}%)\")\n",
    "    print(f\"í™•ë¥ : ì •ìƒ {proba[0]:.2%} | ì•…ì„± {proba[1]:.2%}\")\n",
    "    return prediction\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "test_urls = [\n",
    "    \"https://www.google.com\",\n",
    "    \"http://paypal-secure.com/verify\",\n",
    "    \"http://192.168.1.1/admin\"\n",
    "]\n",
    "\n",
    "for url in test_urls:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    predict_url(url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
