"""
URL í”¼ì‹± íƒì§€ ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
Random Forest ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report
)
import joblib
import os
import sys

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, '/app')

from src.url.feature_extractor import URLFeatureExtractor


# ê²½ë¡œ ì„¤ì •
DATA_DIR = '/app/data/urls'
MODEL_DIR = '/app/models'
os.makedirs(MODEL_DIR, exist_ok=True)


def load_dataset(filepath):
    """ë°ì´í„°ì…‹ ë¡œë“œ"""
    print(f"\nğŸ“‚ ë°ì´í„°ì…‹ ë¡œë“œ: {filepath}")
    df = pd.read_csv(filepath)
    print(f"   - ì „ì²´: {len(df)}ê°œ")
    print(f"   - ì•…ì„±: {(df['label']==1).sum()}ê°œ")
    print(f"   - ì •ìƒ: {(df['label']==0).sum()}ê°œ")
    return df


def extract_features_from_dataset(df):
    """ë°ì´í„°ì…‹ì˜ ëª¨ë“  URLì—ì„œ íŠ¹ì§• ì¶”ì¶œ"""
    print("\nğŸ” íŠ¹ì§• ì¶”ì¶œ ì‹œì‘...")
    
    extractor = URLFeatureExtractor()
    features_list = []
    
    for idx, url in enumerate(df['url']):
        if idx % 10 == 0:
            print(f"   ì§„í–‰: {idx}/{len(df)} ({idx/len(df)*100:.1f}%)", end='\r')
        
        features = extractor.extract_features(url)
        features_list.append(features)
    
    print(f"   ì™„ë£Œ: {len(features_list)}/{len(df)} (100.0%)    ")
    
    # íŠ¹ì§•ì„ DataFrameìœ¼ë¡œ ë³€í™˜
    features_df = pd.DataFrame(features_list)
    
    print(f"\nâœ… íŠ¹ì§• ì¶”ì¶œ ì™„ë£Œ")
    print(f"   - íŠ¹ì§• ê°œìˆ˜: {len(features_df.columns)}ê°œ")
    
    return features_df


def train_model(X_train, y_train, X_test, y_test):
    """Random Forest ëª¨ë¸ í•™ìŠµ"""
    print("\nğŸ¤– ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
    
    # Random Forest ëª¨ë¸ ìƒì„±
    model = RandomForestClassifier(
        n_estimators=100,        # íŠ¸ë¦¬ ê°œìˆ˜
        max_depth=20,            # ìµœëŒ€ ê¹Šì´
        min_samples_split=5,     # ë¶„í•  ìµœì†Œ ìƒ˜í”Œ
        min_samples_leaf=2,      # ë¦¬í”„ ìµœì†Œ ìƒ˜í”Œ
        random_state=42,
        n_jobs=-1,               # ë³‘ë ¬ ì²˜ë¦¬
        verbose=1
    )
    
    # í•™ìŠµ
    model.fit(X_train, y_train)
    
    print("âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")
    
    # í‰ê°€
    print("\nğŸ“Š ëª¨ë¸ í‰ê°€...")
    
    # Train ì„±ëŠ¥
    train_pred = model.predict(X_train)
    train_acc = accuracy_score(y_train, train_pred)
    print(f"\n[Train ì„±ëŠ¥]")
    print(f"   - Accuracy: {train_acc:.4f}")
    
    # Test ì„±ëŠ¥
    test_pred = model.predict(X_test)
    test_acc = accuracy_score(y_test, test_pred)
    test_precision = precision_score(y_test, test_pred)
    test_recall = recall_score(y_test, test_pred)
    test_f1 = f1_score(y_test, test_pred)
    
    print(f"\n[Test ì„±ëŠ¥]")
    print(f"   - Accuracy:  {test_acc:.4f}")
    print(f"   - Precision: {test_precision:.4f}")
    print(f"   - Recall:    {test_recall:.4f}")
    print(f"   - F1-Score:  {test_f1:.4f}")
    
    # Confusion Matrix
    cm = confusion_matrix(y_test, test_pred)
    print(f"\n[Confusion Matrix]")
    print(f"   TN: {cm[0][0]}  FP: {cm[0][1]}")
    print(f"   FN: {cm[1][0]}  TP: {cm[1][1]}")
    
    # ìƒì„¸ ë¦¬í¬íŠ¸
    print(f"\n[Classification Report]")
    print(classification_report(y_test, test_pred, 
                                target_names=['Benign', 'Malicious']))
    
    # íŠ¹ì§• ì¤‘ìš”ë„
    feature_importance = pd.DataFrame({
        'feature': X_train.columns,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print(f"\n[ìƒìœ„ 10ê°œ ì¤‘ìš” íŠ¹ì§•]")
    for idx, row in feature_importance.head(10).iterrows():
        print(f"   {row['feature']:30s}: {row['importance']:.4f}")
    
    return model, feature_importance


def save_model(model, feature_names, metrics):
    """ëª¨ë¸ ë° ë©”íƒ€ë°ì´í„° ì €ì¥"""
    print("\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")
    
    # ëª¨ë¸ ì €ì¥
    model_path = os.path.join(MODEL_DIR, 'url_classifier.pkl')
    joblib.dump(model, model_path)
    print(f"   âœ… ëª¨ë¸: {model_path}")
    
    # íŠ¹ì§• ì´ë¦„ ì €ì¥
    feature_path = os.path.join(MODEL_DIR, 'feature_names.pkl')
    joblib.dump(feature_names, feature_path)
    print(f"   âœ… íŠ¹ì§•: {feature_path}")
    
    # ë©”íƒ€ë°ì´í„° ì €ì¥
    metadata = {
        'model_type': 'RandomForest',
        'n_features': len(feature_names),
        'metrics': metrics,
        'trained_at': pd.Timestamp.now().isoformat()
    }
    
    metadata_path = os.path.join(MODEL_DIR, 'metadata.pkl')
    joblib.dump(metadata, metadata_path)
    print(f"   âœ… ë©”íƒ€ë°ì´í„°: {metadata_path}")
    
    print("\nâœ… ëª¨ë“  íŒŒì¼ ì €ì¥ ì™„ë£Œ!")


def main():
    """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
    print("="*60)
    print("ğŸš€ URL í”¼ì‹± íƒì§€ ëª¨ë¸ í•™ìŠµ ì‹œì‘")
    print("="*60)
    
    # 1. ë°ì´í„° ë¡œë“œ
    dataset_path = os.path.join(DATA_DIR, 'raw', 'sample_dataset.csv')
    
    if not os.path.exists(dataset_path):
        print(f"âŒ ë°ì´í„°ì…‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {dataset_path}")
        print("   ë¨¼ì € ìƒ˜í”Œ ë°ì´í„°ì…‹ì„ ìƒì„±í•˜ì„¸ìš”:")
        print("   python notebooks/00_create_sample_dataset.py")
        return
    
    df = load_dataset(dataset_path)
    
    # 2. íŠ¹ì§• ì¶”ì¶œ
    features_df = extract_features_from_dataset(df)
    
    # íŠ¹ì§• ì €ì¥ (ì„ íƒ)
    features_output = os.path.join(DATA_DIR, 'processed', 'features.csv')
    features_df.to_csv(features_output, index=False)
    print(f"   íŠ¹ì§• ì €ì¥: {features_output}")
    
    # 3. ë°ì´í„° ë¶„í• 
    print("\nğŸ“Š ë°ì´í„° ë¶„í• ...")
    X = features_df
    y = df['label']
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    print(f"   - Train: {len(X_train)}ê°œ")
    print(f"   - Test:  {len(X_test)}ê°œ")
    
    # 4. ëª¨ë¸ í•™ìŠµ
    model, feature_importance = train_model(X_train, y_train, X_test, y_test)
    
    # 5. ëª¨ë¸ ì €ì¥
    test_pred = model.predict(X_test)
    metrics = {
        'accuracy': accuracy_score(y_test, test_pred),
        'precision': precision_score(y_test, test_pred),
        'recall': recall_score(y_test, test_pred),
        'f1_score': f1_score(y_test, test_pred)
    }
    
    save_model(model, list(X.columns), metrics)
    
    print("\n" + "="*60)
    print("ğŸ‰ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
    print("="*60)
    
    # ëª©í‘œ ë‹¬ì„± ì—¬ë¶€
    print(f"\nğŸ¯ ëª©í‘œ ë‹¬ì„± ì—¬ë¶€:")
    print(f"   - Accuracy â‰¥ 85%: {'âœ…' if metrics['accuracy'] >= 0.85 else 'âŒ'} ({metrics['accuracy']:.1%})")
    print(f"   - Recall â‰¥ 90%:   {'âœ…' if metrics['recall'] >= 0.90 else 'âŒ'} ({metrics['recall']:.1%})")


if __name__ == "__main__":
    main()
